{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMxqt+cmw331S2fIKbN/way",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Programlog/MNIST_CNN/blob/main/MNIST_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo9Y8wqn-qxj",
        "outputId": "4acad0bf-3772-4784-9e09-6885a0e5eeb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 18273118.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 495844.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 4495146.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 2754160.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#device configuration\n",
        "device  = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "#hyperparameters\n",
        "epochs = 4\n",
        "batch = 16\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root = './data', train = True, download = True, transform = transforms.ToTensor())\n",
        "test_dataset = torchvision.datasets.MNIST(root = './data', train = False, download = True, transform = transforms.ToTensor())\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch, shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch, shuffle = False)\n",
        "\n",
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        #first layer is a convolution layer, with input size 1 (B&W photo), arbitrary output size 6, kernel size 5x5\n",
        "        self.conv1 = nn.Conv2d(1, 8, 5)\n",
        "\n",
        "        #next is a max pool later, with kernel size 2x2, and stride of 2(shifting two pixels to right)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "\n",
        "        #second layer is another convolution later, with input size 6, output size 16, kernel size 5x5\n",
        "        self.conv2 = nn.Conv2d(8, 16, 5 )\n",
        "\n",
        "        #flatten to allow for linear layers\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        #linear layers\n",
        "        self.fc1 = nn.Linear(16*4*4, 120) # kernel shrinks so new input is now previous output * kernel area\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10) # output nodes should be 10 for the 10 classes (0, 1, 2...)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.pool(F.relu(self.conv1(x)))\n",
        "        # x = self.pool(F.relu(self.conv2(x)))\n",
        "        # x = self.flatten(x)\n",
        "\n",
        "        # x = F.relu(self.fc1(x))\n",
        "        # x = F.relu(self.fc2(x))\n",
        "        # x = self.fc3(x)\n",
        "        # return x\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = CNN().to(device)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Load\n",
        "model.load_state_dict(torch.load('model.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u51gIfiZCEy4",
        "outputId": "cb06711e-be39-4e40-f6cf-6158df9b9a99"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Train\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        #forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # calculate cost\n",
        "        loss = loss_func(outputs, labels)\n",
        "\n",
        "        #backwards pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1)%2000 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item(): .5f}')\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bdd87TCv_xBb",
        "outputId": "d0690ec4-5079-404b-cee3-d9a6b294f76d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/4], Loss:  0.00209\n",
            "Epoch [2/4], Loss:  0.00000\n",
            "Epoch [3/4], Loss:  0.00291\n",
            "Epoch [4/4], Loss:  0.00550\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Save\n",
        "from pathlib import Path\n",
        "\n",
        "model_path = Path('model.pth')\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(\"Saved model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p8WuXQaBVnO",
        "outputId": "aab53f8d-3caf-43ed-e196-0da402b6ab6b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Evaluate\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    n_class_correct = [0 for i in range(10)]\n",
        "    n_class_samples = [0 for i in range(10)]\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += torch.sum(predicted == labels)\n",
        "\n",
        "        for i in range(batch):\n",
        "            label = labels[i]\n",
        "            pred = predicted[i]\n",
        "            if label == pred:\n",
        "                n_class_correct[label] += 1\n",
        "            n_class_samples[label] += 1\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network: {acc: .4f} %')\n",
        "\n",
        "    for i in range(10):\n",
        "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "        print(f'Accuracy of number {classes[i]}: {acc: .4f} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShPX9P1_AZYk",
        "outputId": "596b79c1-0a9b-40dd-dc00-23caa75a0ad6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network:  99.4783 %\n",
            "Accuracy of number 0:  99.2402 %\n",
            "Accuracy of number 1:  98.9321 %\n",
            "Accuracy of number 2:  99.7315 %\n",
            "Accuracy of number 3:  99.6412 %\n",
            "Accuracy of number 4:  99.2811 %\n",
            "Accuracy of number 5:  99.5204 %\n",
            "Accuracy of number 6:  99.6958 %\n",
            "Accuracy of number 7:  99.3136 %\n",
            "Accuracy of number 8:  99.9829 %\n",
            "Accuracy of number 9:  99.5293 %\n"
          ]
        }
      ]
    }
  ]
}